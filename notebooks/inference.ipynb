{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as T\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import (\n",
    "    Accuracy,\n",
    "    F1Score,\n",
    "    JaccardIndex,\n",
    "    Precision,\n",
    "    Recall,\n",
    "    MetricCollection,\n",
    ")\n",
    "from empatches import EMPatches\n",
    "\n",
    "from mine_seg_sat.config import get_model_config\n",
    "from mine_seg_sat.dataset import MineSATDataset\n",
    "from mine_seg_sat.train_utils.utils import get_model, get_loss\n",
    "from mine_seg_sat.train_utils.scale import get_band_norm_values_from_root\n",
    "\n",
    "\n",
    "data_path = Path(\"/mnt/media/data/mine_data/2021/canada/data_dir/mine_dataset\")\n",
    "model_path = Path(\"very_real_path_to_model_weights\")\n",
    "os.environ[\"MODEL_CONFIG\"] = (model_path / \"config.yaml\").as_posix()\n",
    "config = get_model_config()\n",
    "band_meta = get_band_norm_values_from_root(data_path, min_max=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    print(\"Warning: Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([ToTensorV2()], is_check_shapes=False)\n",
    "dataset = MineSATDataset(split=\"test\", data_path=data_path, transformations=transforms, max_values=band_meta[1])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False, num_workers=1)\n",
    "image, label = dataset[0]\n",
    "print(f\"Image and label shapes: {image.shape}, {label.shape}, number of samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchify = EMPatches()\n",
    "img_patches, indices = patchify.extract_patches(image.permute(1, 2, 0).detach().cpu(), patchsize=512, overlap=0.8)\n",
    "img_patches[0].shape, img_patches[1].shape, len(img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = get_loss(config)\n",
    "model = get_model(config, device=device)\n",
    "weights = torch.load((model_path / \"best_model.pth\").as_posix(), map_location=device)\n",
    "new_state_dict = {k.replace('module.', ''): v for k, v in weights[\"model_state_dict\"].items()}\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from itertools import zip_longest\n",
    "from collections import Counter\n",
    "from numpy import prod\n",
    "from functools import partial\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from fvcore.nn.jit_handles import generic_activation_jit\n",
    "\n",
    "def get_shape(val: object) -> typing.List[int]:\n",
    "    \"\"\"\n",
    "    Get the shapes from a jit value object\n",
    "    \"\"\"\n",
    "    if val.isCompleteTensor():\n",
    "        r = val.type().sizes()\n",
    "        if not r:\n",
    "            r = [1]\n",
    "        return r\n",
    "    elif val.type().kind() in (\"IntType\", \"FloatType\"):\n",
    "        return [1]\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "def basic_binary_op_flop_jit(inputs, outputs, name):\n",
    "    input_shapes = [get_shape(v) for v in inputs]\n",
    "    # for broadcasting\n",
    "    input_shapes = [s[::-1] for s in input_shapes]\n",
    "    max_shape = np.array(list(zip_longest(*input_shapes, fillvalue=1))).max(1)\n",
    "    flop = prod(max_shape)\n",
    "    flop_counter = Counter({name: flop})\n",
    "    return flop_counter\n",
    "\n",
    "\n",
    "def pretty_flops(num_flops: int):\n",
    "    \"\"\"\n",
    "    Pretty print the number of FLOPs.\n",
    "    \"\"\"\n",
    "    units = [(\"GFLOPs\", 1e9), (\"MFLOPs\", 1e6), (\"KFLOPs\", 1e3), (\"FLOPs\", 1)]\n",
    "    for unit_name, unit_value in units:\n",
    "        if num_flops >= unit_value:\n",
    "            return f\"{num_flops / unit_value:.2f} {unit_name}\"\n",
    "\n",
    "    return \"0 FLOPs\"\n",
    "\n",
    "\n",
    "input_dim = 128\n",
    "batch_size = 4\n",
    "num_params = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "inputs = torch.randn(2, 12, 512, 512).to(device)\n",
    "\n",
    "counter = FlopCountAnalysis(model, inputs=inputs)\n",
    "counter.set_op_handle(\"aten::softmax\", generic_activation_jit(\"aten::softmax\"))\n",
    "counter.set_op_handle(\"aten::sigmoid\", generic_activation_jit(\"aten::gelu\"))\n",
    "counter.set_op_handle(\"aten::gelu\", generic_activation_jit(\"aten::gelu\"))\n",
    "counter.set_op_handle(\"aten::mish\", generic_activation_jit(\"aten::mish\"))\n",
    "counter.set_op_handle(\"aten::div_\", partial(basic_binary_op_flop_jit, name='aten::div_'))\n",
    "counter.set_op_handle(\"aten::mul\", partial(basic_binary_op_flop_jit, name='aten::mul'))\n",
    "counter.set_op_handle(\"aten::add\", partial(basic_binary_op_flop_jit, name='aten::add'))\n",
    "counter.set_op_handle(\"aten::add_\", partial(basic_binary_op_flop_jit, name='aten::add_'))\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "print(f\"Total number of estimated flops: {pretty_flops(counter.total())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_collection(device: str) -> MetricCollection:\n",
    "    return MetricCollection(\n",
    "        {\n",
    "            \"accuracy\": Accuracy(task=\"multiclass\", num_classes=2, average=None).to(device),\n",
    "            \"f1\": F1Score(task=\"multiclass\", num_classes=2, average=None).to(device),\n",
    "            \"iou\": JaccardIndex(task=\"multiclass\", num_classes=2, average=None).to(device),\n",
    "            \"precision\": Precision(task=\"multiclass\", num_classes=2, average=None).to(device),\n",
    "            \"recall\": Recall(task=\"multiclass\", num_classes=2, average=None).to(device),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = dataset[0]\n",
    "mask = mask.to(device).long()\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "mask = mask.view(1, 1, *mask.shape)\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Image shape: {image.shape}, transformed image shape: {image.permute(1, 2, 0).shape}\")\n",
    "print(f\"Min and max values: {image.min().item()}, {image.max().item()}\")\n",
    "print(f\"Mask values: {mask.min().item()}, {mask.max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    dataset: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    patch_size: int = 512,\n",
    "    num_prints: int = 20,\n",
    ") -> T.Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    prints = 0\n",
    "    metrics = get_metric_collection(device)\n",
    "    with torch.no_grad():\n",
    "        for index in (range(len(dataset))):\n",
    "            image, mask = dataset[index]\n",
    "            mask = mask.to(device).long()\n",
    "            # mask = torch.tensor(mask, device=device).long()\n",
    "            mask = mask.view(1, 1, *mask.shape)\n",
    "            img_patches, img_indices = patchify.extract_patches(image.permute(1, 2, 0).to(device).float(), patchsize=patch_size, overlap=0.8)\n",
    "            outputs = []\n",
    "            for image in img_patches:\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0)\n",
    "                # print(f\"Image shape: {image.unsqueeze(0).shape}, Image min: {image.unsqueeze(0).min()}, Image max: {image.unsqueeze(0).max()}\")\n",
    "                output = model(image)\n",
    "                if (isinstance(output, dict) or isinstance(output, OrderedDict)) and \"out\" in output:\n",
    "                    output = output[\"out\"]\n",
    "                print(f\"output max: {output.max()}, output min: {output.min()}, output shape: {output.shape}\")\n",
    "                prob_mask = output.sigmoid()\n",
    "                print(f\"Mask min: {prob_mask.min()}, Mask max: {prob_mask.max()}\")\n",
    "                outputs.append(prob_mask.squeeze(0).permute((1, 2, 0)).detach().cpu())\n",
    "\n",
    "            # reassemble the patches into a complete image\n",
    "            merged_preds = patchify.merge_patches(outputs, img_indices, \"avg\")\n",
    "            merged_preds = torch.tensor(merged_preds, device=device).float().permute((2, 0, 1)).unsqueeze(0)\n",
    "\n",
    "            metrics.update(merged_preds, mask)\n",
    "            loss += criterion(merged_preds, mask)\n",
    "            if prints < num_prints:\n",
    "                prints += 1\n",
    "                merged_preds = merged_preds.view(merged_preds.shape[2], merged_preds.shape[3])\n",
    "                dataset.display_model_output(index, merged_preds.detach().cpu().numpy())\n",
    "\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "        for name, metric in metrics.items():\n",
    "            print(f\"{name}: {metric.compute()}\")\n",
    "\n",
    "    return metrics.items()\n",
    "\n",
    "results = evaluate_model(dataset, model, loss, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine-seg-sat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
