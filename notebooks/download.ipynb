{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Sentinel-2 Data using the EarthDaily Analytics EarthPlatform\n",
    "\n",
    "This is a template notebook for downloading Sentinel-2 data from the EarthDaily Analytics EarthPlatform and creating tiled files that overlap with environmentally impacted mining sites that have been identified in Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from pystac_client import Client\n",
    "from pystac.item import Item\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.prepared import prep\n",
    "\n",
    "from mine_seg_sat.download.stac import (\n",
    "    get_sentinel2_data,\n",
    "    remove_small_tiles,\n",
    "    add_geometries_iteratively,\n",
    "    download_files_for_item\n",
    ")\n",
    "from mine_seg_sat.utils.bands import get_band_specification\n",
    "from mine_seg_sat.constants import EXTRACTED_BANDS\n",
    "from mine_seg_sat.download.tile import generate_tiles\n",
    "\n",
    "load_dotenv()\n",
    "# Obtain mine shapefile from\n",
    "mine_gdf = gpd.read_file(os.environ.get('thompson_nicola_shp_file_path'))\n",
    "mine_gdf = mine_gdf.to_crs(\"EPSG:4326\")\n",
    "start_date = \"2023-08-10\"\n",
    "end_date = \"2023-08-30\"\n",
    "print(f\"Dropping {len(mine_gdf) - len(mine_gdf[mine_gdf.geometry.is_valid])} invalid geometries.\")\n",
    "mine_gdf = mine_gdf[mine_gdf.geometry.is_valid]\n",
    "#mine_gdf has already been filtered to only contain valid geometries, the second\n",
    "#mine_aoi = mine_gdf[mine_gdf.geometry.is_valid].unary_union\n",
    "mine_aoi = mine_gdf.unary_union\n",
    "\n",
    "len(mine_gdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poetry add geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')  # This moves you up one directory from 'notebooks' to 'your_project'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())  # since your current working directory is now 'your_project'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client_id = os.environ.get('CLIENT_ID')\n",
    "client_secret = os.environ.get('CLIENT_SECRET')\n",
    "auth_token_url = os.environ.get('AUTH_TOKEN_URL')\n",
    "api_url = os.environ.get('API_URL')\n",
    "\n",
    "\n",
    "def get_new_token():\n",
    "    token_req_payload = {'grant_type': 'client_credentials'}\n",
    "    token_response = requests.post(auth_token_url,\n",
    "    data=token_req_payload, verify=False, allow_redirects=False,\n",
    "    auth=(client_id, client_secret))\n",
    "    token_response.raise_for_status()\n",
    "\n",
    "    tokens = json.loads(token_response.text)\n",
    "    return tokens['access_token']\n",
    "\n",
    "\n",
    "token = get_new_token()\n",
    "client = Client.open(api_url, headers={ \"Authorization\": f\"Bearer {token}\" })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_canada_map() -> gpd.GeoDataFrame:\n",
    "    # CRS is EPSG:4326\n",
    "    gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    return gdf[gdf.name == \"Canada\"]\n",
    "\n",
    "areas_geojson = load_canada_map()\n",
    "areas_geojson.iloc[0].geometry\n",
    "#better visulization\n",
    "areas_geojson.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_bounds(geom, delta):\n",
    "    # Convert a larger shapefile into grids...\n",
    "    # Logic retrieved from:\n",
    "    # https://www.matecdev.com/posts/shapely-polygon-gridding.html\n",
    "    minx, miny, maxx, maxy = geom.bounds\n",
    "    nx = int((maxx - minx)/delta)\n",
    "    ny = int((maxy - miny)/delta)\n",
    "    gx, gy = np.linspace(minx,maxx,nx), np.linspace(miny,maxy,ny)\n",
    "    grid = []\n",
    "    for i in range(len(gx)-1):\n",
    "        for j in range(len(gy)-1):\n",
    "            poly_ij = Polygon([[gx[i],gy[j]],[gx[i],gy[j+1]],[gx[i+1],gy[j+1]],[gx[i+1],gy[j]]])\n",
    "            grid.append(poly_ij)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "\n",
    "def partition(geom, delta):\n",
    "    prepared_geom = prep(geom)\n",
    "    grid = list(filter(prepared_geom.intersects, grid_bounds(geom, delta)))\n",
    "    return grid\n",
    "\n",
    "\n",
    "polygons = partition(areas_geojson.iloc[0].geometry, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersecting_polygons(polygons, mine_aoi):\n",
    "    \"\"\"\n",
    "    Given an area with larger multipolygons, and a group of polygons return a list of\n",
    "    polygons that intersect with the area of interest.\n",
    "    \"\"\"\n",
    "    intersecting_polygons = []\n",
    "    for polygon in polygons:\n",
    "        if mine_aoi.intersects(polygon):\n",
    "            intersecting_polygons.append(polygon)\n",
    "    return intersecting_polygons\n",
    "\n",
    "wanted_polygons = get_intersecting_polygons(polygons, mine_aoi)\n",
    "len(polygons), len(wanted_polygons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tiles that intersect with an identified environmentally impacted mining area\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "gpd.GeoSeries(wanted_polygons).boundary.plot(ax=ax)\n",
    "gpd.GeoSeries([areas_geojson.iloc[0].geometry]).boundary.plot(ax=ax, color=\"red\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "###new#####\n",
    "###########\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_polygon(polygon):\n",
    "    poly_obj = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": list(polygon.__geo_interface__[\"coordinates\"])\n",
    "    }\n",
    "    items, tile_gdf = get_sentinel2_data(client, poly_obj, start_date, end_date)\n",
    "    if len(items) == 0:\n",
    "        print(\"No items found for given area... Not great.\")\n",
    "        return None, None\n",
    "\n",
    "    tile_gdf = remove_small_tiles(tile_gdf, reproject=True)\n",
    "    _, tile_gdf = add_geometries_iteratively(tile_gdf)\n",
    "\n",
    "    wanted_gdf = tile_gdf[tile_gdf.intersects(mine_aoi)]\n",
    "    wanted_tiles = [name.split(\"/\")[-1] for name in wanted_gdf[\"earthsearch:s3_path\"].tolist()]\n",
    "    wanted_items = [item for item in items if item.id in wanted_tiles]\n",
    "\n",
    "    return wanted_items, wanted_gdf\n",
    "\n",
    "def get_all_overlapping_tiles(polygons, start_date, end_date):\n",
    "    print(f\"Getting area for: {len(polygons)} polygons\")\n",
    "\n",
    "    all_items = []\n",
    "    all_gdfs = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_polygon, polygons))\n",
    "\n",
    "    for items, gdf in results:\n",
    "        if items is not None:\n",
    "            all_items.append(items)\n",
    "        if gdf is not None:\n",
    "            all_gdfs.append(gdf)\n",
    "\n",
    "    return all_items, all_gdfs\n",
    "\n",
    "all_items, all_gdfs = get_all_overlapping_tiles(wanted_polygons, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_overlapping_tiles(polygons, start_date, end_date):\n",
    "    print(f\"Getting area for: {len(polygons)} polygons\")\n",
    "    all_items, all_gdfs = [], []\n",
    "    for polygon in polygons:\n",
    "        poly_obj = {\n",
    "            \"type\": \"Polygon\",\n",
    "            \"coordinates\": list(polygon.__geo_interface__[\"coordinates\"])\n",
    "        }\n",
    "        items, tile_gdf = get_sentinel2_data(client, poly_obj, start_date, end_date)\n",
    "        if len(items) == 0:\n",
    "            print(\"No items found for given area... Not great.\")\n",
    "            continue\n",
    "\n",
    "        tile_gdf = remove_small_tiles(tile_gdf, reproject=True)\n",
    "        _, tile_gdf = add_geometries_iteratively(tile_gdf)\n",
    "\n",
    "        wanted_gdf = tile_gdf[tile_gdf.intersects(mine_aoi)]\n",
    "        wanted_tiles = [name.split(\"/\")[-1] for name in wanted_gdf[\"earthsearch:s3_path\"].tolist()]\n",
    "        wanted_items = [item for item in items if item.id in wanted_tiles]\n",
    "        all_items.append(wanted_items)\n",
    "        all_gdfs.append(wanted_gdf)\n",
    "\n",
    "    return all_items, all_gdfs\n",
    "\n",
    "\n",
    "all_items, all_gdfs = get_all_overlapping_tiles(wanted_polygons, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [item for sublist in all_items for item in sublist] # covers western canada\n",
    "gdfs = pd.concat(all_gdfs)\n",
    "len(items), len(gdfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_map = mine_gdf.explore(color=\"red\")\n",
    "gdfs.explore(m=mine_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_resolution_bands = {\"red\": \"B04\", \"green\": \"B03\", \"blue\": \"B02\", \"nir\": \"B08\"}\n",
    "mid_resolution_bands = {\"rededge1\": \"B05\", \"rededge2\": \"B06\", \"rededge3\": \"B07\", \"nir08\": \"B8A\", \"swir16\": \"B11\", \"swir22\": \"B12\"}\n",
    "low_resolution_bands = {\"coastal\": \"B01\", \"nir09\": \"B09\"}\n",
    "\n",
    "other_files = {\n",
    "    \"scl\": \"scl\", # Scene Classification Map\n",
    "    \"aot\": \"aot\", # Aerosol Optical Thickness\n",
    "    \"tileinfo_metadata\": \"metadata\" # Tile Metadata\n",
    "}\n",
    "\n",
    "all_download_files = {**high_resolution_bands, **mid_resolution_bands, **low_resolution_bands, **other_files}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "output_dir = Path(os.environ.get('download_file_output_dir'))\n",
    "print(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_tile(tile, output_dir, all_download_files, aoi_gdf):\n",
    "    downloaded_ids = []\n",
    "\n",
    "    try:\n",
    "        dt_obj = datetime.strptime(tile.properties[\"datetime\"], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        formatted_date = dt_obj.strftime('%Y%m%d')\n",
    "        out_path = output_dir / tile.id / formatted_date\n",
    "        downloaded = download_files_for_item(tile, all_download_files, out_path)\n",
    "\n",
    "        if downloaded:\n",
    "            downloaded_ids.append(tile.properties[\"s2:granule_id\"])\n",
    "            for file in out_path.iterdir():\n",
    "                band_name, window_size = get_band_specification(file)\n",
    "                if band_name and window_size:\n",
    "                    out_dir = file.parent / \"tiles\"\n",
    "                    if not out_dir.exists():\n",
    "                        out_dir.mkdir(parents=True)\n",
    "                    generate_tiles(file, out_dir, band_name, window_size, aoi_gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tile {tile.id}: {e}\")\n",
    "\n",
    "    return downloaded_ids\n",
    "\n",
    "def download_and_tile_files(gdf: gpd.GeoDataFrame, items: list[Item], aoi_gdf: gpd.GeoDataFrame, output_dir: Path):\n",
    "    gdf[\"downloaded\"] = False\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers based on your system's capacity\n",
    "        all_downloaded_ids = list(executor.map(lambda tile: process_tile(tile, output_dir, all_download_files, aoi_gdf), items))\n",
    "\n",
    "    for downloaded_ids in all_downloaded_ids:\n",
    "        for id in downloaded_ids:\n",
    "            gdf.loc[gdf[\"s2:granule_id\"] == id, \"downloaded\"] = True\n",
    "\n",
    "    print(f\"Downloaded: {len(gdf[gdf['downloaded'] == True])} / {len(gdf)} files. {len(gdf[gdf['downloaded'] == False])} failed to download.\")\n",
    "load_dotenv()\n",
    "output_dir = Path(os.environ.get('download_file_output_dir'))\n",
    "download_and_tile_files(gdfs, items, mine_gdf, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine-seg-sat-14bnJD6w-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
